Natural language processing is the method of taking a piece of human text, whether it be an article, short phrase, piece of literature or otherwise, and performing a pre-determined task to either classify, or output another piece of text. These tasks can be anything from semantic feedback, to summarising a large piece of literature, or answering questions. Having a machine-language model that can both understand language and infer relevant reasoning from it, can create a powerful bridging gap between humans and the computers that are so abundant around us. 
Natural language processing models have already found their way into most homes and are more prominent in our day-to-day life than perhaps most people realise. Advancements in electronic personal assistants both in standalone devices such as the Amazon Echo or Google Nest, or systems embedded in our smartphones like Apple’s Siri and Google Assistant have established the first step into enabling a user to directly communicate with a computer system, where the spoken word can be just as powerful a tool as typing a command into a system directly. This helps give powerful access to a generation of users that use a tool such as this as part of their everyday existence, rather than a far-reaching, complex process like in the first era of computers where only the tech savvy could exploit a machine’s full potential.
This field isn’t without its limitations however; until recently, most NLP applications utilised recurrent neural networks, where the output of one iteration is fed back into the model as the input for the next iteration, useful for when the entire sequence of an input is more important than any one single item in the sequence, thus acting as a sort of internal memory, this being where the LSTM (Long-Short Term Memory) model gets its name. This internal memory essentially allows for previous words in a sentence to become useful when trying to predict the next, as intuitively, it’s much easier to know what comes next when you know what came before. These models suffer greatly however when a sentence or paragraphs context is given by a word much earlier in the sequence, where the weight of this word’s significance is drowned out by the many that came after. The transformer model discussed in this report was created to address these issues directly and has become the leading standard for natural language processing.
As advancements in Artificial Intelligence and Machine Learning are only speeding up, many employees have become worried their own job may be at risk of being made redundant by a machine that can potentially save the company much more money in the long term. Through this research I explore methods of how to integrate these tools as part of the workforce, rather than replacing it, and whether such tools have already had success. I also address any social limitations to these systems and why employers and homeowners may still not be ready to accept them into their everyday life. Finally, political impacts will also be discussed, providing insight as to why governments are restricting access to some of the tools to the general public. 
Before these socio-economic impacts can be explored, technical details of the Transformer will be provided. These details help give insight to limitations, the mathematical approaches used, and lay the foundation for the rest of the project, which will study the major three Transformer models being used today, BERT, T5, and GPT-3, and where they are currently being applied.  
